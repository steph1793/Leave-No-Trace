# Leave-No-Trace
:wink: :put_litter_in_its_place: Learning to Reset for Safe and Autonomous Reinforcement Learning

We based our work on this [paper](https://arxiv.org/abs/1711.06782). The authors aimed to provide a framework for safe learning, allowing our agent to learn good behaviour (quickly) and run for longer episodes avoiding numerous resets which can be costly.


## How to run the code
We can launch the project  by running the notebook on google colab. You have two main sections : The experiments section and the results section. In the Notebook we clone the repository. if you want to visualize the results, you can go directly to the results section(after installing and importing the dependencies). And if you want to launch you own experiments, you need to rename or remove the old results folder.


## Our experiments
We train our agent to walk using a Soft Actor Critic method. We experimented this method alone and we also tested the Leave-No-Trace.

## Highlight Some results

![Sac only](https://raw.githubusercontent.com/steph1793/Leave-No-Trace/master/results_viz/sac_only.gif)
